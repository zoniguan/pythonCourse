#### Thinking1	什么是反向传播中的链式法则
- 正向传播是传递输出信号, 反向传播是传递输出的误差
- 反向传播是通过使用矩阵相乘的形式, 求解隐藏层的误差
- 链式法则是指, 输出层误差在转置权重矩阵的帮助下,传递到了隐藏层, 用来更新与隐藏层相连的权重矩阵
  
  

#### Thinking2	请列举几种常见的激活函数，激活函数有什么作用
- 常见的激活函数: sigmod函数(取值范围是(0,1) ); tanh函数(取值范围是(-1,1) ), 以及使用比较普遍的ReLu函数(双向函数, 小于0是为0, 大于0时为输入本身)
- 激活函数相当于将输出层信息(点积的结果)作为输入, 经过非线性的激活函数, 输出到下一层隐藏层; 如果没有激活函数, 输出层与输出层的关系都是线性关系, 而引入激活函数, 可以提高神经网络的表达能力. 才能发挥隐藏层的作用.


#### Thinking3	利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？
- 模型的loss不变, 主要是模型没有经过更新, 即权重没有发生重置. 
- 导致这种情况是由于发生了梯度消失. 当梯度非常小(小于1), 预测值与真实值之间的误差每传播一层就会衰减(小于1的数相乘, 结果更小), 尤其是使用sigmod函数作为激活函数时, 会导致模型收敛停止; 而发生梯度消失时, 对离输出层越远(即离输入层更近)的隐藏层, 会有输入层权值更新缓慢或者停滞,从而loss模型不变.
- 可以尝试替换激活函数, 如Relu函数. 因为ReLu的非负区间梯度为常数, 不存在梯度消失的问题.使得模型的收敛速度维持在一个比较稳定的状态. 也所以Relu函数比较受大家常用为激活函数的原因.





