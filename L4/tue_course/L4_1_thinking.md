### Thinking1	ALS都有哪些应用场景	

- ALS算法描述：
    1. ALS算法用来补全用户评分矩阵。由于用户评分矩阵比较稀疏，将用户评分矩阵进行分解，变成V和U的乘积。通过求得V和U两个小的矩阵来补全用户评分矩阵。
    2. ALS算法使用交替最小二乘法来进行求解。
    3. ALS分为显示反馈和隐式反馈两种。显示反馈是指用户有明确的评分。对于商品推荐来说，大部分是通过用户的行为，获取隐式反馈的评分。
    4. 隐式反馈评分矩阵需要进行处理，如果有用户评分则置为1，没有则赋值为0。但是对这个处理后的评分矩阵，再有一个置信度来评价这个评分。置信度等于1+a*用户真实评分
    5. ALS的代价函数是估计值和现有的评分值误差的平方和，引入了L2正则

- ALS属于数据挖掘,可以做推荐系统,比如电影推荐,商品推荐,广告推荐等.
    - 原理就是给各个指标,判定等加权重,然后将这些训练集输入ALS,包括其他的参数,内部进行矩阵相乘,根据这些权重,给用户对未知,未点击的商品也给一个分数,就是喜好程度
    - 然后把喜好程度高的商品推荐给用户,假如用户不喜欢,从线上观察效果不好,那这个模型就有问题,需要修改参数,修改权重,或者添加权重,使之达到一个理想的效果.




### Thinking2	ALS进行矩阵分解的时候，为什么可以并行化处理	
- ALS交替最小二乘法求解步骤:
    1. 最小二乘法是通过最小化误差的平方和来寻找和数据最匹配的函数。
    2. 步骤是先设置一个X规定，然后求解另一个矩阵Y。然后再固定一个矩阵Y，求解另一个矩阵X。这就是交替二乘法的步骤。
- 在矩阵求解的过程中，比如固定Y，求解X的话，目标评分矩阵A。X的每一行可以独立求解，X的第i行和Y的计算得到A的第i行。
- 所以ALS可以并行化处理


### Thinking3	梯度下降法中的批量梯度下降（BGD），随机梯度下降（SGD），和小批量梯度下降有什么区别（MBGD）
1. 批量梯度下降(Batch Gradient Descent，BGD):
    - 使用整个训练集的优化算法被称为批量(batch)或确定性(deterministic)梯度算法，因为它们会在一个大批量中同时处理所有样本。
    - 批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。
    - 优点:
        - 在训练过程中，使用固定的学习率，不必担心学习率衰退现象的出现。
        - 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，一定能收敛到全局最小值，如果目标函数非凸则收敛到局部最小值。
        - 它对梯度的估计是无偏的。样例越多，标准差越低。
        - 一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行。
    - 缺点：
        - 计算需要大量的时间
        - 每次的更新都是在遍历全部样例之后发生的，这时才会发现一些例子可能是多余的且对参数更新没有太大的作用。

2. 随机梯度下降(Stochastic Gradient Descent，SGD):
    - 随机梯度下降法不同于批量梯度下降，随机梯度下降是在每次迭代时使用一个样本来对参数进行更新（mini-batch size =1）。
    - 优点：
        - 在学习过程中加入了噪声，提高了泛化误差。
        - 由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快
    - 缺点：
        - 不收敛，在最小值附近波动。
        - 不能在一个样本中使用向量化计算，学习过程变得很慢。
        - 单个样本并不能代表全体样本的趋势。
        - 当遇到局部极小值或鞍点时，SGD会卡在梯度为0处。

3. 小批量梯度下降(Mini-batch Gradient Descent，MBGD):
    - 使用一个以上而又不是全部的训练样本。
    - 在算法的每一步，我们从具有m个样本的训练集（已经打乱样本的顺序）中随机抽出一小批量(mini-batch)样本; 小批量的数目m'通常是一个相对较小的数（从1到几百）。重要的是，当训练集大小m增长时，m'通常是固定的。
    - 优点:
        - 计算速度比BGD快，因为只遍历部分样例就可执行更新。
        - 随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例。
        - 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。
    - 缺点：
        - 在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛。
        - 学习过程会有更多的振荡，为更接近最小值，需要增加学习率衰减项，以降低学习率，避免过度振荡

















### Thinking4	你阅读过和推荐系统/计算广告/预测相关的论文么？有哪些论文是你比较推荐的，可以分享到微信群中	












